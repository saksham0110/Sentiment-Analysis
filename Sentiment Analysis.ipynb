{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is the process of analyzing the natural human language and predicting the sentiment of some text. Predicting \n",
    "whether the text has a positive or negative or neutral sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk is a platform that helps write python programs for analyzing natural language. It provides with various libraries such as tokenizing, stemming, parsing, etc. nltk provides some of the sample tweets for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "RT @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain £170 billion per year! #BetterOffOut #UKIP\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweets[0])\n",
    "print()\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing: \n",
    "It's a process of seperating text into seperate tokens (different words, emoticons, links, @mentions, etc.). It helps to \n",
    "classify the words and understand it's meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punkt is a library which helps to tokenize tweets data into tokens to make it easier for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization: \n",
    "Normalization is a process of converting a word into its canonical form. \n",
    "Grouping of words together which means same meaning but are in different forms.\n",
    "Example: run, running, ran.\n",
    "\n",
    "There are two popular methods of performing normalization in nlp\n",
    "1. Stemming\n",
    "2. Lemmatization\n",
    "\n",
    "Stemming: \n",
    "wikipedia - stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n",
    "Stemming is method used to normalize text by removing the affixes of a word. \n",
    "\n",
    "Lemmatization:\n",
    "wikipedia - Lemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item.\n",
    "Lemmatization is similar to stemming but it brings context to the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet: wordnet is the lexical database, an english dictionary specifically designed for nlp. It helps to determine the base word.\n",
    "\n",
    "Averaged_perceptron_tagger: It helps to apply parts of speech to the text. Used to determine the context of a word in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging: Tagging is the process of assigning a word to a part of speech.\n",
    "List of all tags used in pos_tag : https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "pos_tag: Library used to determine the context of a word by analyzing it's relative position and tagging it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "print(pos_tag(tweet_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "print(lemmatize_sentence(tweet_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Noise:\n",
    "\n",
    "Noise is anything in a text that doesn't matter or adds no meaning to it. Therefore it's better to remove noise from the text.\n",
    "For removing noise we will use regex library. Documentation: https://docs.python.org/3/library/re.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words=()):\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        #removing hyperlinks\n",
    "        token = re.sub(r'^https?:\\/\\/.*[\\r\\n]*','', token)\n",
    "        #removing @ mentions\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(remove_noise(tweet_tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht'] ['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_token_list = []\n",
    "negative_cleaned_token_list = []\n",
    "\n",
    "for i in positive_tweet_tokens:\n",
    "    positive_cleaned_token_list.append(remove_noise(i, stop_words))\n",
    "for j in negative_tweet_tokens:\n",
    "    negative_cleaned_token_list.append(remove_noise(j, stop_words))\n",
    "    \n",
    "# Comparison between the original and cleaned tweets.\n",
    "print(positive_tweet_tokens[500], positive_cleaned_token_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Word Density:\n",
    "\n",
    "To find the frequency of each word in all the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_all_words(cleaned_token_list):\n",
    "    for tweet in cleaned_token_list:\n",
    "        for token in tweet:\n",
    "            yield token\n",
    "# yield is keyword that is used to return multiple values on the execution of this generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = compile_all_words(positive_cleaned_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253), ('u', 245), ('day', 242), ('like', 229), ('see', 195), ('happy', 192), (\"i'm\", 183), ('great', 175), ('hi', 173), ('go', 167), ('back', 163)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "freq_pos_words = FreqDist(positive_words)\n",
    "print(freq_pos_words.most_common(20))\n",
    "# or\n",
    "# print(sorted(freq_pos_words)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9966666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2058.0 : 1.0\n",
      "                      :) = True           Positi : Negati =   1660.5 : 1.0\n",
      "                follower = True           Positi : Negati =     25.2 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.5 : 1.0\n",
      "                     sad = True           Negati : Positi =     17.7 : 1.0\n",
      "                  arrive = True           Positi : Negati =     15.7 : 1.0\n",
      "                    damn = True           Negati : Positi =     15.5 : 1.0\n",
      "                     x15 = True           Negati : Positi =     15.5 : 1.0\n",
      "                followed = True           Negati : Positi =     14.9 : 1.0\n",
      "              appreciate = True           Positi : Negati =     13.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_token_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_token_list)\n",
    "\n",
    "positive_dataset = [(i, \"Positive\") for i in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(j, \"Negative\") for j in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on random text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text1 = 'The movie got a good start and seemed to be very interesting. I would rate this movie four stars:). Great characters and acting'\n",
    "text2 = 'The story had a confusing plot and their was no chemistry between the characters :/. It could have a better usage of language and vocabulary too.'\n",
    "\n",
    "text1 = remove_noise(word_tokenize(text1))\n",
    "text2 = remove_noise(word_tokenize(text2))\n",
    "print(classifier.classify(dict([token,True] for token in text1)), classifier.classify(dict([token,True] for token in text2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps Performed:\n",
    "1. Tokenize text\n",
    "2. Normalize text\n",
    "3. Remove noise\n",
    "4. Transform data\n",
    "5. Training and testing sets\n",
    "6. Train model\n",
    "7. Test model\n",
    "\n",
    "It was simple, wasn't it :).\n",
    "\n",
    "# Conclusion:\n",
    "This was our classifier model (naive bayes classifier) which helps to identify the sentiment of a text (It may be a review about a movie or a product or a general tweet). We used only positive and negative sentiments to make it easier to understand. \n",
    "You may classify a text to be positive, negative. You can have your own list of positive, negative and neutral words to impliment the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Thanks:\n",
    "All the credits goes to https://www.digitalocean.com/community/users/sdaityari for making it simple to understand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
